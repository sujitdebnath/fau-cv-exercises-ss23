{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import cv2\n",
    "from typing import Tuple, Dict, List\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are typehints, they mostly make the code readable and testable\n",
    "t_points = np.array\n",
    "t_descriptors = np.array\n",
    "t_homography = np.array\n",
    "t_img = np.array\n",
    "t_images = Dict[str, t_img]\n",
    "t_homographies = Dict[Tuple[str, str], t_homography]  # The keys are the keys of src and destination images\n",
    "\n",
    "np.set_printoptions(edgeitems=30, linewidth=180,\n",
    "                    formatter=dict(float=lambda x: \"%8.05f\" % x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img: t_img, num_features: int = 500) -> Tuple[t_points, t_descriptors]:\n",
    "    \"\"\"Extract keypoints and their descriptors.\n",
    "    The OpenCV implementation of ORB is used as a backend.\n",
    "    https://en.wikipedia.org/wiki/Oriented_FAST_and_rotated_BRIEF\n",
    "\n",
    "    Args:\n",
    "        img: a numpy array of [H x Wx 3] size with byte values.\n",
    "        num_features: an integer signifying how many points we desire.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing a numpy array of [N x 2] and numpy array of [N x 32]\n",
    "    \"\"\"\n",
    "    #TODO : Hint - you will need cv2.ORB_create\n",
    "    #raise NotImplementedError\n",
    "    orb = cv2.ORB_create(nfeatures=num_features, scoreType=cv2.ORB_FAST_SCORE)\n",
    "    kp, des = orb.detectAndCompute(img, None)\n",
    "\n",
    "    kp,des = np.array([p.pt for p in kp]).astype(np.double), np.array(des).astype(np.double)\n",
    "    return (kp,des)\n",
    "\n",
    "def filter_and_align_descriptors(f1: Tuple[t_points, t_descriptors], f2: Tuple[t_points, t_descriptors],\n",
    "                                 similarity_threshold=.7, similarity_metric='hamming') -> Tuple[t_points, t_points]:\n",
    "    \"\"\"Aligns pairs of keypoints from two images.\n",
    "    Aligns keypoints from two images based on descriptor similarity.\n",
    "    If K points have been detected in image1 and J points have been detected in image2, the result will be to sets of N\n",
    "    points representing points with similar descriptors; where N <= J and K <=points.\n",
    "\n",
    "    Args:\n",
    "        f1: A tuple of two numpy arrays with the first array having dimensions [N x 2] and the second one [N x M]. M\n",
    "            representing the dimensionality of the point features. In the case of ORB features, M is 32.\n",
    "        f2: A tuple of two numpy arrays with the first array having dimensions [J x 2] and the second one [J x M]. M\n",
    "            representing the dimensionality of the point features. In the case of ORB features, M is 32.\n",
    "        similarity_threshold: The ratio the distance of most similar descriptor in image2 to the distance of the second\n",
    "            most similar ratio.\n",
    "        similarity_metric: A string with the name of the metric by witch distances are calculated. It must be compatible\n",
    "            with the ones that are defined for scipy.spatial.distance.cdist.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of numpy arrays both sized [N x 2] representing the similar point locations.\n",
    "\n",
    "    \"\"\"\n",
    "    assert f1[0].dtype == f2[0].dtype == np.double\n",
    "    assert f1[0].shape[1] == f2[0].shape[1] == 2  # descriptor size\n",
    "    assert f1[1].shape[1] == f2[1].shape[1] == 32  # points size\n",
    "\n",
    "    # step 1: compute distance matrix (1 to 8 lines)\n",
    "    N = f1[0].shape[0]\n",
    "    distance_matrix = np.zeros([N,N]).astype(np.float32)\n",
    "    #print(f1[1][0])\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            #des1, des2 = (f1[1]>0.5).astype(np.uint8), (f2[1]>0.5).astype(np.uint8)\n",
    "            dist = cv2.norm(f1[1][i], f2[1][j], cv2.NORM_HAMMING)\n",
    "            distance_matrix[i,j] = dist\n",
    "    print(distance_matrix.shape)\n",
    "\n",
    "    # step 2: computing the indexes of src dst so that src[src_idx,:] and dst[dst,:] refer to matching points.\n",
    "    src_idx = np.argmin(distance_matrix, axis=1)\n",
    "    dst_idx = np.arange(len(f2[0]))\n",
    "\n",
    "    # step 3: find a boolean index of the matched pairs that is true only if a match was significant.\n",
    "    # A match is considered significant if the ratio of it's distance to the second best is lower than a given\n",
    "    # threshold.\n",
    "    # Hint: use the previously computed distance matrix to find the second best match.\n",
    "\n",
    "    sorted_distances = np.sort(distance_matrix, axis=1)\n",
    "    ratios = sorted_distances[:, 0] / sorted_distances[:, 1]\n",
    "    significant_matches = ratios < similarity_threshold\n",
    "\n",
    "    # step 4: removing non significant matches and return the aligned points (their location only!)\n",
    "    aligned_points_src = f1[0][src_idx[significant_matches]]\n",
    "    aligned_points_dst = f2[0][dst_idx[significant_matches]]\n",
    "\n",
    "    #ratio_threshold=0.6\n",
    "    #outlier_indices = np.where(ratios >= ratio_threshold)\n",
    "    #aligned_points_src = np.delete(aligned_points_src, outlier_indices, axis=0)\n",
    "    #aligned_points_dst = np.delete(aligned_points_dst, outlier_indices, axis=0)\n",
    "\n",
    "\n",
    "    return aligned_points_src, aligned_points_dst\n",
    "\n",
    "\n",
    "    #raise NotImplementedError\n",
    "\n",
    "def compute_homography(f1: np.array, f2: np.array) -> np.array:\n",
    "    \"\"\"Computes the homography matrix given matching points.\n",
    "\n",
    "    In order to define a homography a minimum of 4 points are needed but the homography can also be overdefined with 5\n",
    "    or more points.\n",
    "\n",
    "    Args:\n",
    "        f1: A numpy array of size [N x 2] containing x and y coordinates of the source points.\n",
    "        f2: A numpy array of size [N x 2] containing x and y coordinates of the destination points.\n",
    "\n",
    "    Returns:\n",
    "        A [3 x 3] numpy array containing normalised homography matrix.\n",
    "    \"\"\"\n",
    "    # Homogeneous coordinates\n",
    "    homography_matrix = np.zeros((3, 3))\n",
    "    assert f1.shape[0] == f2.shape[0] >= 4\n",
    "\n",
    "    # TODO 3\n",
    "    # - Construct the (>=8) x 9 matrix A.\n",
    "    # - Use the formula from the exercise sheet.\n",
    "    # - Note that every match contributes to exactly two rows of the matrix.\n",
    "    # - Extract the homogeneous solution of Ah=0 as the rightmost column vector of V.\n",
    "    # - Store the result in H.\n",
    "    # - Normalize H\n",
    "    # Hint: No loops are needed but upto 2 nested loops might make the solution easier.\n",
    "    N = f1.shape[0]\n",
    "    A = np.zeros((2*N,9))\n",
    "    for i in range(N):\n",
    "        x,y = f1[i]\n",
    "        xp, yp = f2[i]\n",
    "        A[2*i] = [-x, -y, -1, 0, 0, 0, x*xp, y*xp, xp]\n",
    "        A[2*i + 1] = [0, 0, 0, -x, -y, -1, x*yp, y*yp, yp]\n",
    "    \n",
    "    _,_,V = np.linalg.svd(A,full_matrices=True)\n",
    "    V = V.T\n",
    "    h = V[:,-1]\n",
    "    homography_matrix = h.reshape(3,3)\n",
    "    homography_matrix = homography_matrix*(1/h[-1])\n",
    "\n",
    "    return homography_matrix\n",
    "\n",
    "def _get_inlier_count(src_points: np.array, dst_points: np.array, homography: np.array,\n",
    "                      distance_threshold: float) -> int:\n",
    "    \"\"\"Computes the number of inliers for a homography given aligned points.\n",
    "    ## - Project the image points from image 1 to image 2\n",
    "    ## - A point is an inlier if the distance between the projected point and\n",
    "    ##      the point in image 2 is smaller than threshold.\n",
    "    Args:\n",
    "        src_points: a numpy array of [N x 2] containing source points.\n",
    "        dst_points: a numpy array of [N x 2] containing source points.\n",
    "        homography: a [3 x 3] numpy array.\n",
    "        distance_threshold: a float representing the norm of the difference between to points so that they will be\n",
    "            considered the same (near enough).\n",
    "\n",
    "    Returns:\n",
    "        An integer counting how many transformed source points matched destination.\n",
    "    \"\"\"\n",
    "    assert src_points.shape[1] == dst_points.shape[1] == 2\n",
    "    assert src_points.shape[0] == dst_points.shape[0]\n",
    "\n",
    "    peers = list(zip(src_points, dst_points))\n",
    "    inlierCount =0 \n",
    "    for points_matches in peers:\n",
    "        q = list(points_matches[1]) # point in image 2\n",
    "        q.append(1)\n",
    "        p = list(points_matches[0]) #point in image 1\n",
    "        p.append(1)\n",
    "        approximation_ = np.linalg.norm(np.array(q).reshape(-1,1) - np.dot(homography, np.array(p).reshape(-1,1)),ord=1)\n",
    "        #print(approximation_)\n",
    "        if approximation_ < distance_threshold:\n",
    "            inlierCount += 1\n",
    "    return inlierCount\n",
    "\n",
    "\n",
    "def ransac(src_features: Tuple[t_points, t_descriptors], dst_features: Tuple[t_points, t_descriptors], steps,\n",
    "           distance_threshold, n_points=4, similarity_threshold=.7) -> np.array:\n",
    "    \"\"\"Computes the best homography given noisy point descriptors.\n",
    "\n",
    "    https://en.wikipedia.org/wiki/Random_sample_consensus\n",
    "    \n",
    "    Args:\n",
    "        src_features: A tuple with points and their descriptors detected in the source image.\n",
    "        dst_features: A tuple with points and their descriptors detected in the destination image.\n",
    "        steps: An integer defining how many iterations to define.\n",
    "        distance_threshold: A float defining how far should to points be to be considered the same.\n",
    "        n_points: The number of point pairs used to compute the homography, it must be grater than 3.\n",
    "        similarity_threshold: The ratio of the most similar descriptor to the second most similar in order to consider\n",
    "            that descriptors from the two images match.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array containing the homography.\n",
    "    \"\"\"\n",
    "\n",
    "    # step 1: filter and align descriptors (1 line)\n",
    "    src_points, dst_points = filter_and_align_descriptors(src_features,dst_features,similarity_threshold)\n",
    "\n",
    "    # step 2: initialize the optimization loop\n",
    "    best_count = 0\n",
    "    best_homography = np.eye(3)\n",
    "\n",
    "    # step 3: optimization loop\n",
    "    for n in range(steps):\n",
    "        if n == steps - 1:\n",
    "            print(f\"Step: {n:4}  {best_count} RANSAC points match!\")\n",
    "        # step a: select random subset of points (atleast 4 points) (2 lines)\n",
    "        indices = np.random.choice(range(len(src_points)), size=4, replace=False)\n",
    "        #print(indices)\n",
    "        src, dst = src_points[indices], dst_points[indices]\n",
    "\n",
    "        # step b: compute homography for the random points (1 line)\n",
    "        h = compute_homography(src, dst)\n",
    "\n",
    "        # step c: compare the current homography to the current best homography and update the best homography using\n",
    "        # inlier count (4 lines)\n",
    "        current_inlier_count = _get_inlier_count(src, dst, h, distance_threshold)\n",
    "        print(current_inlier_count)\n",
    "        if current_inlier_count > best_count:\n",
    "            best_count = current_inlier_count\n",
    "            best_homography = h\n",
    "\n",
    "\n",
    "    # step 4: return the best homography\n",
    "    return best_homography\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Step:    9  0 RANSAC points match!\n",
      "0\n",
      "[[-151.23725 36.67990 130.74473]\n",
      " [-27.31265 10.22763 118.09432]\n",
      " [-0.04234 -0.31017  1.00000]]\n",
      "[[ 1.00000  0.00000  0.00000]\n",
      " [ 0.00000  1.00000  0.00000]\n",
      " [ 0.00000  0.00000  1.00000]]\n"
     ]
    }
   ],
   "source": [
    "def test_ransac():\n",
    "    N = 256\n",
    "    h = np.array([[-151.2372466105457, 36.67990057507507, 130.7447340624461],\n",
    "                      [-27.31264543681857, 10.22762978292494, 118.0943169422209],\n",
    "                      [-0.04233528054472634, -0.3101691983762523, 1]])\n",
    "\n",
    "    points = np.random.rand(N, 2) * 100\n",
    "    descriptors = np.random.rand(N, 32)\n",
    "\n",
    "    moved_points = cv2.perspectiveTransform(points[None, :, :], h)[0, :, :]\n",
    "\n",
    "    #  Making half the points outliers\n",
    "    location_noise = np.random.rand(N, 2) * 200 - 10\n",
    "    outlier = (np.random.rand(N) > .9)[:, None]\n",
    "    moved_points = moved_points + outlier * location_noise\n",
    "\n",
    "    new_h = ransac((points, descriptors), (moved_points, descriptors), steps=10, distance_threshold=.5)\n",
    "    print(h)\n",
    "    print(new_h)\n",
    "    #assert all_similar(h, new_h)\n",
    "\n",
    "test_ransac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probagate_homographies(homographies: t_homographies, reference_name: str) -> t_homographies:\n",
    "    \"\"\"Computes homographies from every image to the reference image given a homographies between all pairs of\n",
    "    consecutive images.\n",
    "\n",
    "    This method could be loosely described as applying Dijkstra's algorithm applied to exploit the commutative\n",
    "    relationship of matrix multiplication and compute homography matrices between all images and any image.\n",
    "\n",
    "    Args:\n",
    "        homographies: A dictionary where the keys are tuples with the names of each image pair and the values are\n",
    "            [3 x 3] arrays containing the homographies between those images.\n",
    "        reference_name: The of the image which will be the destination for all homographies.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of the same form as the input mappning all images to the reference.\n",
    "    \"\"\"\n",
    "    initial = {k: v for k, v in homographies.items()}  # deep copy\n",
    "    for k, h in list(initial.items()):\n",
    "        initial[(k[1], k[0])] = np.linalg.inv(h)\n",
    "    initial[(reference_name, reference_name)] = np.eye(3)  # Added the identity homography for the reference\n",
    "    desired = set([(k[0], reference_name) for k in homographies.keys()])\n",
    "    solved = {k: v for k, v in initial.items() if k[1] == reference_name}\n",
    "    while not (set(solved.keys()) >= desired):\n",
    "\n",
    "        new_steps = set([(i, s) for i, s in product(initial.keys(), solved.keys()) if\n",
    "                     s[1] != i[0] and s[0] == i[1] and s[0] != s[1] and (i[0], s[1]) not in solved.keys()])\n",
    "        # s[1] != i[0] no pair who's product leads to identity\n",
    "        # s[0] == i[1] only connected pairs\n",
    "        # s[0]!=s[1] no identity in the solution\n",
    "        # set removes duplicates\n",
    "\n",
    "        assert len(new_steps) > 0  # not all desired can be linked to reference\n",
    "        for initial_k, solved_k in new_steps:\n",
    "            new_key = initial_k[0], solved_k[1]\n",
    "            solved[solved_k]\n",
    "            initial[initial_k]\n",
    "            solved[new_key] = np.matmul(solved[solved_k], initial[initial_k])\n",
    "    return solved\n",
    "\n",
    "\n",
    "def compute_panorama_borders(images: t_images, homographies: t_homographies) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"Computes the bounding box of the panorama defined the images and the homographies mapping them to the reference.\n",
    "\n",
    "    This bounding box can have non integer and even negative coordinates.\n",
    "\n",
    "    Args:\n",
    "        images: A dictionary mapping image names to numpy arrays containing images.\n",
    "        homographies:  A dictionary mapping Tuples with pairs image names to numpy arrays representing homographies\n",
    "            mapping from the first image to the second.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the bounding box [left, top, right, bottom] of the whole panorama if stiched.\n",
    "\n",
    "    \"\"\"\n",
    "    homographies = {k[0]: v for k, v in homographies.items()}  # assining homographies to their source image\n",
    "    assert homographies.keys() == images.keys()  # map homographies to source image only\n",
    "    all_corners = []\n",
    "    for name in sorted(images.keys()):\n",
    "        img, homography = images[name], homographies[name]\n",
    "        width, height = img.shape[0], img.shape[1]\n",
    "        corners = ((0, 0), (0, width), (height, width), (height, 0))\n",
    "        corners = np.array(corners, dtype='float32')\n",
    "        all_corners.append(cv2.perspectiveTransform(corners[None, :, :], homography)[0, :, :])\n",
    "    all_corners = np.concatenate(all_corners, axis=0)\n",
    "    left, right = np.floor(all_corners[:, 0].min()), np.ceil(all_corners[:, 0].max())\n",
    "    top, bottom = np.floor(all_corners[:, 1].min()), np.ceil(all_corners[:, 1].max())\n",
    "    return left, top, right, bottom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_homographies(homographies: t_homographies, dx: float, dy: float):\n",
    "    \"\"\"Applies a uniform translation to a dictionary with homographies.\n",
    "\n",
    "    Args:\n",
    "        homographies: A dictionary mapping Tuples with pairs image names to numpy arrays representing homographies\n",
    "            mapping from the first image to the second.\n",
    "        dx: a float representing the horizontal displacement of the translation.\n",
    "        dy: a float representing the vertical displacement of the translation.\n",
    "\n",
    "    Returns:\n",
    "        a copy of the homographies dict which maps the same keys to the translated matrices.\n",
    "    \"\"\"\n",
    "    # step 1: create a translation matrix (3 lines)\n",
    "    translation_matrix = np.eye(3)\n",
    "    translation_matrix[:,-1] = [dx, dy, 1]\n",
    "\n",
    "    # step 2: apply translation matrix on every homography matrix (2 lines)\n",
    "    homographies_copy = {}\n",
    "    for key in homographies.keys():\n",
    "        homographies_copy[key] = np.dot(translation_matrix, homographies[key])\n",
    "\n",
    "    return homographies_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_panorama(images: t_images, homographies: t_homographies, output_size: Tuple[int, int],\n",
    "                   rendering_order: List[str] = []) -> t_images:\n",
    "    \"\"\"Stiches images after it reprojects them with a homography.\n",
    "\n",
    "    Args:\n",
    "        images: A dictionary mapping image names to numpy arrays containing images.\n",
    "        homographies: A dictionary mapping Tuples with pairs image names to numpy arrays representing homographies\n",
    "            mapping from the first image to the reference image.\n",
    "        output_size: A tuple with integers representing the witdh and height of the resulting panorama.\n",
    "        rendering_order: A list containing the names of the images representing the order in witch the images will be\n",
    "            overlaid. The list must contain either all images names in some permutation or be empty in which case, the\n",
    "            images will be rendered in the alphanumeric order of their names.\n",
    "    Returns:\n",
    "        A numpy array with the panorama image.\n",
    "    \"\"\"\n",
    "    homographies = {k[0]: v for k, v in homographies.items()}  # assining homographies to their source image\n",
    "    assert homographies.keys() == images.keys()\n",
    "    if rendering_order == []:\n",
    "        rendering_order = sorted(images.keys())\n",
    "    panorama = np.zeros([output_size[1], output_size[0], 3], dtype=np.uint8)\n",
    "    for name in rendering_order:\n",
    "        rgba_img = cv2.cvtColor(images[name], cv2.COLOR_RGB2RGBA)\n",
    "        rgba_img[:, :, 3] = 255\n",
    "        tmp = cv2.warpPerspective(rgba_img, homographies[name], output_size, cv2.INTER_LINEAR_EXACT)\n",
    "        new_pixels = ((tmp[:, :, 3] == 255)[:, :, None] & (panorama == np.zeros([1, 1, 3])))\n",
    "        old_pixels = 1 - new_pixels\n",
    "        panorama[:, :, :] = panorama * old_pixels + tmp[:, :, :3] * new_pixels\n",
    "    return panorama\n",
    "\n",
    "\n",
    "def create_stitched_image(images: t_images, homographies: t_homographies, reference_name: str,\n",
    "                          rendering_order: List[str] = []):\n",
    "    \"\"\"Will create a panorama by stitching the input images after reprojecting them.\n",
    "\n",
    "    Args:\n",
    "        images: A dictionary mapping image names to numpy arrays containing images.\n",
    "        homographies: A dictionary mapping Tuples with pairs image names to numpy arrays representing homographies\n",
    "            that can reproject the first image to be aligned with the reference image.\n",
    "        reference_name: A string with the name of the image to which all other images will be aligned.\n",
    "        rendering_order: A list containing the names of the images representing the order in witch the images will be\n",
    "            overlaid. The list must contain either all images names in some permutation or be empty in which case, the\n",
    "            images will be rendered in the alphanumeric order of their names.\n",
    "    Returns:\n",
    "        A numpy array with the panorama image.\n",
    "    \"\"\"\n",
    "    #  from homographies between consecutive images we compute all homographies from any image to the reference.\n",
    "    homographies = probagate_homographies(homographies, reference_name=reference_name)\n",
    "    #  lets calculate the panorama size\n",
    "    left, top, right, bottom = compute_panorama_borders(images, homographies)\n",
    "    width = int(1 + np.ceil(right) - np.floor(left))\n",
    "    height = int(1 + np.ceil(bottom) - np.floor(top))\n",
    "    #  lets make the homographies translate all images inside the panorama.\n",
    "    homographies = translate_homographies(homographies, -left, -top)\n",
    "    return stitch_panorama(images, homographies, (width, height), rendering_order=rendering_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=np.array([[2,3,4],[4,5,2],[5,7,8]])\n",
    "#t = t[:,:,np.newaxis]\n",
    "t[:,-1] = [2,1,4]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = cv2.imread('/Users/awritrojitbanerjee/FAU/Sem2/CV/Ex/submission_ex2/data/ex2/0.jpg')#.astype(np.float32)\n",
    "img1 = cv2.imread('/Users/awritrojitbanerjee/FAU/Sem2/CV/Ex/submission_ex2/data/ex2/1.jpg')#.astype(np.float32)\n",
    "img2 = cv2.imread('/Users/awritrojitbanerjee/FAU/Sem2/CV/Ex/submission_ex2/data/ex2/2.jpg')#.astype(np.float32)\n",
    "img3 = cv2.imread('/Users/awritrojitbanerjee/FAU/Sem2/CV/Ex/submission_ex2/data/ex2/3.jpg')#.astype(np.float32)\n",
    "img4 = cv2.imread('/Users/awritrojitbanerjee/FAU/Sem2/CV/Ex/submission_ex2/data/ex2/4.jpg')#.astype(np.float32)\n",
    "img5 = cv2.imread('/Users/awritrojitbanerjee/FAU/Sem2/CV/Ex/submission_ex2/data/ex2/5.jpg')#.astype(np.float32)\n",
    "img6 = cv2.imread('/Users/awritrojitbanerjee/FAU/Sem2/CV/Ex/submission_ex2/data/ex2/6.jpg')#.astype(np.float32)\n",
    "img7 = cv2.imread('/Users/awritrojitbanerjee/FAU/Sem2/CV/Ex/submission_ex2/data/ex2/7.jpg')#.astype(np.float32)\n",
    "img8 = cv2.imread('/Users/awritrojitbanerjee/FAU/Sem2/CV/Ex/submission_ex2/data/ex2/8.jpg')#.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_homographies = [i for i in itertools.combinations(range(9),r=2)]\n",
    "imglist = [img0,img1,img2,img3,img4,img5,img6,img7,img8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "homographies = {}\n",
    "\n",
    "for img_pair in num_homographies:\n",
    "\n",
    "    img_a, img_b = imglist[img_pair[0]], imglist[img_pair[1]]\n",
    "\n",
    "    kpa, desa = extract_features(img_a, num_features=10)\n",
    "    kpb, desb = extract_features(img_b, num_features=10)\n",
    "\n",
    "    h_ab = compute_homography(kpa,kpb)\n",
    "\n",
    "    homographies[(str(img_pair[0]), str(img_pair[1]))] = h_ab\n",
    "\n",
    "images = { str(i):imglist[i] for i in range(len(imglist)) }\n",
    "refname = \"0\"\n",
    "rendering_order = [str(i) for i in range(len(imglist))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "panorama = create_stitched_image(images, homographies, refname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(panorama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp0, des0 = extract_features(img0,50)\n",
    "kp1, des1 = extract_features(img1,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.drawKeypoints(img0, [cv2.KeyPoint(point[0],point[1], size=20) for point in kp0], None, color=(0,255,0), flags=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-6\n",
    "\n",
    "\n",
    "def all_similar(t1, t2):\n",
    "    \"\"\"Test the maximum square error is lesser than epsilon.\"\"\"\n",
    "    delta = (t1 - t2) ** 2\n",
    "    correct = delta > epsilon\n",
    "    return correct.reshape(-1).mean() == 0\n",
    "\n",
    "points1src = np.array(((1, 1), (3, 7), (2, -5), (10, 11)))\n",
    "points1dst = np.array(((25, 156), (51, -83), (-144, 5), (345, 15)))\n",
    "points2src = np.array(((1, 1), (1, 1), (3, 7), (2, -5), (10, 11)))\n",
    "points2dst = np.array(((25, 156), (25, 156), (51, -83), (-144, 5), (345, 15)))\n",
    "points3src = np.repeat(points2src, 100, axis=0) + np.random.rand(500, 2) * .00001\n",
    "points3dst = np.repeat(points2dst, 100, axis=0) + np.random.rand(500, 2) * .00001\n",
    "\n",
    "\n",
    "def testHomography():\n",
    "    points1 = [(1, 1), (3, 7), (2, -5), (10, 11)]\n",
    "    points2 = [(25, 156), (51, -83), (-144, 5), (345, 15)]\n",
    "\n",
    "    H = compute_homography(np.array(points1), np.array(points2))\n",
    "\n",
    "    print (\"Testing Homography...\")\n",
    "    print (\"Your result:\" + str(H))\n",
    "\n",
    "    Href = np.array([[-151.2372466105457,   36.67990057507507,   130.7447340624461],\n",
    "                 [-27.31264543681857,   10.22762978292494,   118.0943169422209],\n",
    "                 [-0.04233528054472634, -0.3101691983762523, 1]])\n",
    "\n",
    "    print (\"Reference: \" + str(Href))\n",
    "\n",
    "    error = Href - H\n",
    "    e   = np.linalg.norm(error)\n",
    "    print (\"Error: \" + str(e))\n",
    "\n",
    "    if (e < 1e-10):\n",
    "        print (\"Test: SUCCESS!\")\n",
    "    else:\n",
    "        print (\"Test: FAIL!\")\n",
    "    print (\"============================\")\n",
    "testHomography()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
